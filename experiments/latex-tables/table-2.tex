\begin{table}

\experimentTableSize

\begin{tabular}{l|cccccc}
& \multicolumn{3}{c}{\textbf{Experiment 2a}}
& \multicolumn{3}{c}{\textbf{Experiment 2b}} \\\hline
\textbf{Name} &
\textbf{Expert} & \textbf{Expert} & \textbf{Expert} &
\textbf{Random} & \textbf{Random} & \textbf{Random} \\
&
\textit{Submission:} & \multicolumn{2}{c}{\textit{Revised Artifact:}} &
\textit{Submission:} & \multicolumn{2}{c}{\textit{Revised Artifact:}} \\
&
\textit{Fig. 10} & \textit{Ours} & \textit{Yours} &
\textit{Fig. 10} & \textit{Ours} & \textit{Yours} \\
\input{generated/table-2-data}
\end{tabular}

\vspace{0.10in}

\caption{Experiment 2.
%
Differences (in blue) between results from \snsMyth{} at submission and
\snsMyth{} now:
%
\experimentCaptionSize
%
\\[3pt]
%
\textbf{list\_compress, tree\_binsert, tree\_nodes\_at\_level:} Not run because
they failed in Experiment 1.
%
\\[3pt]
%
\textbf{Expert: bool\_xor:} With the algorithmic changes, \snsMyth{} now
requires (all) 4 examples.  (Small changes to search order and search parameters
can change the results of synthesis tools.)
%
\\[3pt]
%
\textbf{Expert: list\_filter:} The Experiment 1 expert examples were extended by
one; the Experiment 2 expert examples were, too.
%
\\[3pt]
%
\textbf{Expert: list\_snoc, list\_take:} When looking through our tasks again,
we noticed an opportunity to try removing another example from these benchmarks;
\snsMyth{} produces correct solutions given the fewer examples.
%
\\[3pt]
%
\textbf{Random:} Small variations in k50 and k90 are expected because the
examples are generated randomly. There are some blue dots and dashes because our
scripts for benchmarking and generating the table differences do not
automatically display {\scriptsize{failed}}, {\scriptsize{timeout}},
superscripts 3 and 10, or the $\downarrow$ arrow presented in Figure 10.
%
\\[3pt]
%
}

\end{table}
